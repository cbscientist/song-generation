{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(folder_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Given a directory of text files, builds a corpus of\n",
    "    sentences that can be used to train neural networks.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                full_path_to_file = root + \"/\" + filename\n",
    "                with open(full_path_to_file, 'r') as infile:\n",
    "                    lines = infile.read().split('\\n')\n",
    "                    corpus += lines\n",
    "    \n",
    "    # deduplicating because I'm getting weird prediction results\n",
    "    corpus = list(set(corpus))\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = build_corpus(\"./corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for an OOV token, since we'll be training using the entire corpus\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "tfidf_scores = tokenizer.sequences_to_matrix(tokenizer.texts_to_sequences(corpus), mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scores = pd.DataFrame(tfidf_scores, columns = ['unknown'] + list(tokenizer.word_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "foobar = KMeans(n_clusters=10).fit(my_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 2, 2, ..., 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foobar.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1549, 1722)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scores.loc[:, 'Cluster'] = foobar.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_01 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==0].index)]\n",
    "cluster_02 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==1].index)]\n",
    "cluster_03 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==2].index)]\n",
    "cluster_04 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==3].index)]\n",
    "cluster_05 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==4].index)]\n",
    "cluster_06 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==5].index)]\n",
    "cluster_07 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==6].index)]\n",
    "cluster_08 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==7].index)]\n",
    "cluster_09 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==8].index)]\n",
    "cluster_10 = [corpus[i] for i in list(my_scores[my_scores['Cluster']==9].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_9' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fh/3s2jb7fs7t1dxbsh_lbp1r2c0000gn/T/ipykernel_66671/2348843663.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster_9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_9' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc1b947dce198ff7f2d2cb152b2cbb61132fce4429fa808fd5b89ac4d7df39fa"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
